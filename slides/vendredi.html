<section>
  <h1>Apache Spark</h1>
  <h2>Qu'est-ce que c'est ?</h2>
</section>

<section>
  <h3>En deux mots : </h3>
  <ul>
    <li>Framework de calcul distribué</li>
    <li>Tolérant à la panne</li>
    <li>Performant (tout <em>In Memory</em>, à l'inverse de <em>Map Reduce</em>)</li>
    <li>Possibilité d'utiliser du SQL pour manipuler les données (qui peuvent être issues de base hétérogènes)</li>
  </ul>
  <aside class="notes">
    Schéma d'un ETL distribué
  </aside>
</section>

<section>
  <h3>Le projet <em>Tungsten</em></h3>
  <ul>
    <li>Initié avec Spark 1.4</li>
    <li><em>CPU is the bottlneck</em></li>
    <li>Depuis Spark 2.0, génération de code à la volée</li>
  </ul>
  <aside class="notes">
    Schéma avec SELECT<br />
    Suite aux bench de Databricks : CPU is the bottelneck
  </aside>
</section>

<section>
  <h1>WTF?!</h1>
  <aside class="notes">
    Image WTF ? <br />
    Pour bien comprendre, on farfouille dans Spark <br />
    Qu'est ce que ça veut dire génération de code à la volée ?
  </aside>
</section>

<section>
  <h2>Génération de code à la volée</h2>
  <ul>
    <li>Execution d'un code compilé qui n'est pas défini à la compilation</li>
    <li>Partir d'un <code>String</code> pour récupérer une <code>Class[_]</code></li>
    <li class="fragment">Essayons !</li>
  </ul>
</section>

<section>
  <pre class="bigger-code scala"><code data-trim data-noescape>
val javaClassCode =
    """public class Talker {
      |
      | public String talk() {
      |   return "Hello, World! ";
      | }
      |
      |}
      |
    """.stripMargin
  </code></pre>
  <aside class="notes">
    Pourquoi Java ? On a voulu faire comme Spark : c'est plus rapide à compiler, et en plus avec Janino, c'est stylé parce que c'est tout en mémoire.
  </aside>
</section>

<section>
  <pre class="bigger-code bash"><code data-trim data-noescape>
import sys.process._

Seq(
  "javac",
  "-d", "/tmp/generated-sources",
  "/tmp/generated-classes"
) !
  </code></pre>
</section>

<section>
  <pre class="bigger-code scala"><code data-trim data-noescape>
val classLoader = new URLClassLoader("/tmp/generated-classes")

val myTalker: Any = classLoader
  .loadClass("Talker")
  .newInstance()
  </code></pre>
</section>

<section>
  <div>Ouch</div>
  <div>La variable <code>myTaker</code> est de type <code>Any</code></div>
  <div>Pour appeler plus facilement la méthode <code>talk()</code>, on rajoute une interface</div>
  <pre class="bigger-code java"><code data-trim data-noescape>
public interface Talker {
    String talk();
}
  </code></pre>
  <pre class="bigger-code scala"><code data-trim data-noescape>
  val javaClassCode =
    """public class GeneratedTalker implements Talker {
      |
      | @Override
      | public String talk() {
      |   return "Hello, World! ";
      | }
      |
      |}
      |
    """.stripMargin
  </code></pre>
</section>

<section>
  <div>Et voilà le travail !</div>

  <pre class="bigger-code scala"><code data-trim data-noescape>
val myTalker: Talker = classLoader
  .loadClass("HelloTalker")
  .newInstance()
  .asInstanceOf[Talker]
  </code></pre>
  <pre class="bigger-code scala fragment"><code data-trim data-noescape>
println(myTalker.talk())
// "Hello World !"
  </code></pre>
</section>

<section>
  <img src="./images/compileJavaClean.png" style="background:none; border:none; box-shadow:none;"/>
</section>

<section>
  <h1>So what?</h1>
  <aside class="notes">
    Bon bah c'est super, on sait générer du code... Et maintenant, on fait quoi ?
  </aside>
</section>

<section>
  <h1>Execution d'une requête SQL</h1>
  <pre class="bigger-code scala fragment"><code data-trim data-noescape>
def execute: String => Seq[Row] = ???
  </code></pre>
  <aside class="notes">
    Retour au point de départ : on a une String, co
  </aside>
</section>

<section>
  <aside class="notes">
    Résumé de tout sous forme de schéma
  </aside>
</section>

<section>
  <h1>Le parsing</h1>
</section>

<section>
  <h2>Le Lexing</h2>

  <aside class="notes">
    Schéma avec un exemple de requête SQL
  </aside>
</section>

<section>
  <pre class="bigger-code scala fragment"><code data-trim data-noescape>
sealed trait Token

case class Select() extends Token

case class From() extends Token

// ...
  </code></pre>
  <pre class="bigger-code scala fragment"><code data-trim data-noescape>
def lexer: String => Seq[Token] = ???
  </code></pre>
</section>

<section>
  <h2>Le parsing</h2>

  <aside class="notes">
    Pareil, un schéma depuis les token vers l'AST
  </aside>
</section>

<section>
  <pre class="bigger-code scala fragment"><code data-trim data-noescape>
trait AST

case class Table(name: String) extends AST

case class Alias(table: Table, name: String) extends AST

case class Select(
  projections: Seq[Expression],
  filter: Option[Expression],
  relations: Seq[Relation]
) extends AST

// ...

trait Expression extends AST

// ...
  </code></pre>

  <pre class="bigger-code scala fragment"><code data-trim data-noescape>
def parser: Seq[Token] => AST
  </code></pre>
  <aside class="notes">
    Le parser permet de donner un sens aux tokens : de hiérarchiser les tokens, et contextualiser tout ça. <br />
    N'importe quel language utilise ça (y a un AST pour Java, pour Scala, etc.) y a rien de révolutionnaire et ça n'est pas du tout propre à Spark
  </aside>
</section>

<section>
  <aside class="notes">
    Logos de Parser Combinator et ANTLR <br />
    Pour nous, les Parser Combinator s'était suffisant, mais ANTLR c'est plus sérieux pour des langages beaucoup plus complexes (type SQL ANSI, etc.)
  </aside>
</section>

<section>
  <h2>Ce qu'il faut retenir</h2>
  <ul>
      <li>Grâce au parser, on a :</li>
      <li><ul>
        <li>Donné du sens au <code>String</code></li>
        <li>Un moyen d'exploiter la requête de manière programatique grâce à l'<code>AST</code></li>
      </ul></li>
  </ul>
  Il ne reste plus qu'à exécuter...
</section>

<section>
  <h1>Execution</h1>
</section>

<section>
  <h2>Catalyst</h2>
  <aside class="notes">
    C'est le moteur SQL, donc il gère la génération des plans et aussi l'optimisation
  </aside>
</section>

<section>
  <p>Deux étapes : </p>
  <ul>
    <li>Génération d'un plan d'execution logique : quelles sont les étapes par lesquelles passer pour évaluer l'AST</li>
    <li>Génération d'un plan d'execution physique : comment est-ce que l'on executer ses étapes en fonction de l'environnement </li>
  </ul>
  <aside class="notes">
    Avec Spark, tu peux faire des jointures entre des sources de données hétérogène. <br />
    D'un point de vue logique, tu fais des jointures entre table, mais d'un point de vue physique, tu vas faire une jointure entre CsvTable et une JdbcTable
    On en reparlera plus tard, mais un join logique peut-être un hash Join ou un nested loop (suivant l'environnemtn dans lequel tu executes)
    Le plan d'execution logique, il ne permet pas d'executer, c'est juste une vision algébrique de ta requête (c'est vraiment un plan)
    Le plan physique, c'est vraiment l'execution (le marteau piqueur, celui qui rentre dans le dur)
    Ça n'est absolument pas spécifique à Spark, c'est commun à toutes les base de données (Oracle, etc.)
  </aside>
</section>

<section>
  <pre class="bigger-code scala"><code data-trim data-noescape>
sealed trait LogicalPlan

case class Projection(
  child: LogicalPlan,
  expression: Seq[Expression]
) extends LogicalPlan

case class Join(
  leftChild: LogicalPlan,
  rightChild: LogicalPlan,
  expression: BinaryOperation
) extends LogicalPlan

case class Filter(
  child: LogicalPlan,
  expression: Expression
) extends LogicalPlan

// ...
  </code></pre>
</section>

<section>
  <pre class="bigger-code scala fragment"><code data-trim data-noescape>
trait PhysicalPlan {

  def execute(): Iterator[Row]
}

case class CSVFileFullScan(
  qualifierName: String,
  csvFileURL: URL
) extends PhysicalPlan {

  override def execute(): Iterator[Row] = ???

}

case class Filter(
  child: PhysicalPlan,
  expression: e.Expression
) extends PhysicalPlan {

  override def execute() : Iterator[Row] = ???

}
  </code></pre>
</section>

<section>
  <h2>Le <em>Volcano Model</em></h2>
  <p>Un opérateur = Une classe</p>
</section>

<section>
  <aside class="notes">
    Image du volcano model <br />
    Classe qui s'imbriquent
  </aside>
</section>

<section>
  <h2>Catalyst</h2>
  <aside class="notes">
    Optimisation
  </aside>
</section>
<section>
  <h2>Quelques optimisations dans Spark</h2>
  <ul>
    <li><code>PushDownPredicate</code></li>
    <li><code>ColumnPruning</code></li>
    <li><code>CombineFilters</code></li>
  </ul>
</section>
<section>
  <img src="./images/catalystExample.png" style="background:none; border:none; box-shadow:none;"/>
</section>
<section>
  <h2>Une structure en arbre idéale pour agencer et optimiser le plan d'exécution</h2>
</section>
<section>
  <h2>Définition d'un <em>Trait</em></h2>
  <pre class="bigger-code scala"><code data-trim data-noescape>
trait TreeNode[Type <: TreeNode[Type]] {
                       self: Type =>

  def children: Seq[Type]

  def copyWithNewChildren(children: Seq[Type]): Type = ???

  def mapChildren(f: Type => Type): Type = {
    copyWithNewChildren(children.map(f))
  }

  def transformDown(pf: PartialFunction[Type, Type]): Type = {
    pf.applyOrElse(this, identity[Type])
      .mapChildren(_.transformDown(pf))
  }

}
  </code></pre>
</section>
<section>
  <h2>Exemple d'optimisation</h2>
  <pre class="bigger-code scala"><code data-trim data-noescape>
object CombineFilters extends Rule[LogicalPlan] {
  def apply(plan: LogicalPlan): LogicalPlan = plan transform {
    case Filter(exp1, Filter(exp2, grandChild)) =>
          Filter(And(exp1, exp2), grandChild)
  }
}
  </code></pre>
</section>

