<section>
  <header>
    <p>Génération de code, moteur <em>Catalyst</em>&hellip;</p>
    <h1>Démystifions <em>Apache Spark !</em></h1>
  </header>
</section>

<section>
  <img src="./images/apache-spark-logo.svg" />
  <h2>Qu'est-ce que c'est ?</h2>
</section>

<section>
  <h3>En résumé, <em>Apache Spark</em>&nbsp;:</h3>
  <ul>
    <li>Est un <strong>framework de calcul distribué</strong> <em>In Memory</em>&nbsp;;</li>
    <li>Qui est <strong>tolérant à la panne</strong>&nbsp;;</li>
    <li>Se base sur des <strong>abstractions</strong> de plusieurs niveaux (<code>RDD[_]</code>, <code>DataFrame</code>, etc.)&nbsp;;</li>
    <li>Permet l'<strong>utilisation du SQL</strong> sur des sources de données hétérogènes. </li>
  </ul>
  <aside class="notes">
    Schéma d'un ETL distribué
  </aside>
</section>

<section>
  <h3>Le projet <em>Tungsten</em></h3>
  <ul>
    <li>Initié à partir d'<em>Apache Spark</em> 1.4&nbsp;;</li>
    <li>Ayant pour objectif une <strong>amélioration des performances</strong> sans changement de l'API&nbsp;;</li>
    <li><blockquote>CPU is the new bottlneck</blockquote>&nbsp;;</li>
    <li>Depuis <em>Apache Spark</em> 2.0, <strong>génération de code</strong> à la volée. </li>
  </ul>
  <aside class="notes">
    Schéma avec SELECT<br />
    Suite aux bench de Databricks : CPU is the bottelneck
  </aside>
</section>

<section>
  <img src="./images/wait-a-minute.jpg" />
  <aside class="notes">
    Image WTF ? <br />
    Pour bien comprendre, on farfouille dans Spark <br />
    Qu'est ce que ça veut dire génération de code à la volée ?
  </aside>
</section>

<section>
  <h2>Générer du code à la volée ?</h2>
  <p>Challenge Accepted! </p>
</section>

<section>
  <h3>De quoi parle-t-on exactement ?</h3>
  <ul>
    <li>Compiler un code <strong>pendant l'exécution</strong>&nbsp;;</li>
    <li>Partir d'un <code>String</code> pour récupérer une <code>Class[_]</code>&nbsp;;</li>
  </ul>
  <pre class="bigger-code scala"><code data-trim data-noescape>
def compile(code: String): Class[_] = ???
  </code></pre>
  <h3 class="fragment">Essayons !</h3>
</section>

<section>
  <pre class="bigger-code scala"><code data-trim data-noescape>
val javaClassCode =
    """public class Talker {
      |
      | public String talk() {
      |   return "Hello, World! ";
      | }
      |
      |}
      |
    """.stripMargin
  </code></pre>
  <pre class="bigger-code scala fragment"><code data-trim data-noescape>
import sys.process._

Seq(
  "javac",
  "-d", "/tmp/generated-sources",
  "/tmp/generated-classes"
) !
  </code></pre>
  <pre class="bigger-code scala fragment"><code data-trim data-noescape>
val classLoader = new URLClassLoader("/tmp/generated-classes")
  </code></pre>
</section>

<section>
  <pre class="bigger-code scala"><code data-trim data-noescape>
val myTalker = classLoader
  .loadClass("Talker")
  .newInstance()
myTalker.talk()
  </code></pre>
  <pre class="bigger-code scala fragment"><code data-trim data-noescape>
// Cannot resolve symbol talk
  </code></pre>
</section>

<section>
  <h3>Ouch! </h3>
  <p>Le type de <code>myTalker</code> n'est pas défini à la compilation !</p>
  <p>Il nous faut un <strong>pont</strong> entre le <em>Compile Time</em> et le <em>Run Time</em>&hellip; </p>
  <pre class="bigger-code java fragment"><code data-trim data-noescape>
public interface Talker {
    String talk();
}
  </code></pre>
  <aside class="notes">
    Pourquoi Java ? On a voulu faire comme Spark : c'est plus rapide à compiler, et en plus avec Janino, c'est stylé parce que c'est tout en mémoire.
  </aside>
</section>

<section>
  <pre class="bigger-code scala"><code data-trim data-noescape>
  val javaClassCode =
    """public class GeneratedTalker implements Talker {
      |
      | @Override
      | public String talk() {
      |   return "Hello, World! ";
      | }
      |
      |}
      |
    """.stripMargin
  </code></pre>
  <pre class="bigger-code scala fragment"><code data-trim data-noescape>
val myTalker: Talker = classLoader
  .loadClass("HelloTalker")
  .newInstance()
  .asInstanceOf[Talker]
  </code></pre>
  <pre class="bigger-code scala fragment"><code data-trim data-noescape>
println(myTalker.talk())
// "Hello World !"
  </code></pre>
</section>

<section>
  <img src="./images/compileJava.png" style="background:none; border:none; box-shadow:none;"/>
</section>

<section>
  <h3>Et <em>Apache Spark</em>, dans tout ça ?</h2>
  <img src="./images/so-what.png" />
  <aside class="notes">
    Bon bah c'est super, on sait générer du code... Et maintenant, on fait quoi ?
  </aside>
</section>

<section>
  <h3>MythBuster: Apache Spark</h3>
  <img src="./images/mythbuster-apache-spark-logo.png" />
  <p>Où comment passer 6 mois à décortiquer le code d'<em>Apache Spark</em>&hellip;</p>
</section>

<section>
  <h2><em>Back To Basics...</em></h2>
  <p>Comment <em>Apache Spark</em> exécute une requête SQL&nbsp;?</p>
  <pre class="bigger-code scala fragment"><code data-trim data-noescape>
def execute(sql: String): Seq[Row] = ???
  </code></pre>
  <aside class="notes">
    Retour au point de départ : on a une String, co
  </aside>
</section>

<section>
  <h3><em>Parsing</em> d'une reqête SQL</h3>
  <p>Deux étapes&nbsp;:</p>
  <img src="?" />
  <aside class="notes">
    Résumé de tout sous forme de schéma
  </aside>
</section>

<section>
  <h4>L'analyse lexicale</h4>
  <aside class="notes">
    Schéma avec un exemple de requête SQL
  </aside>
</section>

<section>
  <pre class="bigger-code scala"><code data-trim data-noescape>
sealed trait Token

case class Select() extends Token

case class From() extends Token

// ...
  </code></pre>
  <pre class="bigger-code scala fragment"><code data-trim data-noescape>
def lexer: String => Seq[Token] = ???
  </code></pre>
</section>

<section>
  <h4>L'analyse syntaxique</h4>

  <aside class="notes">
    Pareil, un schéma depuis les token vers l'AST
  </aside>
</section>

<section>
  <pre class="bigger-code scala fragment"><code data-trim data-noescape>
trait AST

case class Table(name: String) extends AST

case class Alias(table: Table, name: String) extends AST

case class Select(
  projections: Seq[Expression],
  filter: Option[Expression],
  relations: Seq[Table]
) extends AST

// ...

trait Expression extends AST

// ...
  </code></pre>

  <pre class="bigger-code scala fragment"><code data-trim data-noescape>
def parser: Seq[Token] => AST
  </code></pre>
  <aside class="notes">
    Le parser permet de donner un sens aux tokens : de hiérarchiser les tokens, et contextualiser tout ça. <br />
    N'importe quel language utilise ça (y a un AST pour Java, pour Scala, etc.) y a rien de révolutionnaire et ça n'est pas du tout propre à Spark
  </aside>
</section>

<section>
  <h3>Implémentations</h3>
  <div>
    <div class="left-column">
      <img src="./images/scala-logo.png" />
      <h4 style="text-align: center;">Parser Combinators</h4>
    </div>
    <div class="right-column">
      <img style="width: 50%; height: 50%;" src="./images/antlr-logo.jpg" />
    </div>
  </div>
  <aside class="notes">
    Logos de Parser Combinator et ANTLR <br />
    Pour nous, les Parser Combinator s'était suffisant, mais ANTLR c'est plus sérieux pour des langages beaucoup plus complexes (type SQL ANSI, etc.)
  </aside>
</section>

<section>
  <h3>Ce qu'il faut retenir&nbsp;:</h3>
  <ul>
    <li>L'analyse lexicale transforme une chaîne de caractère en lexèmes appartenant au <strong>lexique du langage</strong>&nbsp;;</li>
    <li>L'analyse syntaxique <strong>donne du sens</strong> à ces lexèmes en produisant un <em>AST</em>&nbsp;</li>
    <li>L'<em>AST</em> est une structure exploitable par le code&nbsp;</li>
    <li><strong>ANTLR est un standard</strong> pour des langages complexes, et c'est ce qu'utilise <em>Apache Spark</em>. </li>
  </ul>
  <p><span class="fragment">Notre requête SQL étant parsée&hellip; </span><span class="fragment">Il n'y plus qu'à l'executer&nbsp;!</span></p>
</section>

<section>
  <h3>Exécution de la requête SQL</h3>
  <p>Deux étapes&nbsp;</p>
</section>

<section>
  <h4>Plan d'execution logique</h4>
  <p>Quelles sont les étapes par lesquelles on va passer pour évaluer l'AST&nbsp;?</p>
  <pre class="bigger-code scala"><code data-trim data-noescape>
sealed trait LogicalPlan
  </code></pre>
  <aside class="notes">
    Avec Spark, tu peux faire des jointures entre des sources de données hétérogène. <br />
    D'un point de vue logique, tu fais des jointures entre table, mais d'un point de vue physique, tu vas faire une jointure entre CsvTable et une JdbcTable
    On en reparlera plus tard, mais un join logique peut-être un hash Join ou un nested loop (suivant l'environnemtn dans lequel tu executes)
    Le plan d'execution logique, il ne permet pas d'executer, c'est juste une vision algébrique de ta requête (c'est vraiment un plan)
    Le plan physique, c'est vraiment l'execution (le marteau piqueur, celui qui rentre dans le dur)
    Ça n'est absolument pas spécifique à Spark, c'est commun à toutes les base de données (Oracle, etc.)
  </aside>
</section>

<section>
  <pre class="bigger-code scala"><code data-trim data-noescape>
case class Projection(
  child: LogicalPlan, expression: Seq[Expression]
) extends LogicalPlan

case class Join(
  leftChild: LogicalPlan, rightChild: LogicalPlan,
  expression: BinaryOperation
) extends LogicalPlan

case class Filter(
  child: LogicalPlan,
  expression: Expression
) extends LogicalPlan

// ...
  </code></pre>
  <p class="fragment">Purement déclaratif, toujours pas d'execution !</p>
</section>

<section>
  <h4>Plan d'exécution physique</h4>
  <p>Maintenant que l'on connait les étapes par lesquelles passées, comment est-ce qu'on les exécute&nbsp;?</p>
</section>

<section>
  <h4>Le <em>Volcano Model</em></h4>
  <p>Un peu d'histoire&nbsp;:</p>
  <ul>
    <li>Aussi appelé l'<em>Iterator Model</em>&nbsp;</li>
    <li>La quasi-totalité des SGBDR s'appuient sur ce fonctionnement (PostgreSQL, Oracle, MySQL, etc.)&nbsp;</li>
    <li><em>Push</em> de la donnée&nbsp;</li>
    <li>Un opérateur&nbsp;&harr;&nbsp;Une classe. </li>
  </ul>
  <pre class="bigger-code scala"><code data-trim data-noescape>
trait PhysicalPlan {

  def execute(): Iterator[Row]
}
  </code></pre>
</section>

<section>
  <pre class="bigger-code scala"><code data-trim data-noescape>
case class Filter(
  child: PhysicalPlan, predicate: Expression with Predicate
) extends PhysicalPlan {

  override def execute() : Iterator[Row] = {
    child.execute()
      .filter(predicate.evaluate)
  }

}
  </code></pre>
  <pre class="bigger-code scala fragment"><code data-trim data-noescape>
case class CSVFileFullScan(
  csvFilePath: Path
) extends PhysicalPlan

case class NestedLoopJoin(
  leftChild: PhysicalPlan,
  rightChild: PhysicalPlan,
  predicate: Expression with Predicate
) extends PhysicalPlan

// Etc.
  </code></pre>
</section>

<section>
  <aside class="notes">
    Image du volcano model <br />
    Classe qui s'imbriquent
  </aside>
</section>

<section>
  <img src="./images/résumé.png" />
  <aside class="notes">

  </aside>
</section>

<section>
  <h3>Optimisation</h3>
  <p>Objectif&nbsp;:</p>
  <ul>

    <li>Au sein d'<em>Apache Spark</em>, le moteur d'execution s'appelle <em>Catalyst</em>. </li>
  </ul>
  <aside class="notes">
Définition +

  </aside>
</section>

<section>
  <h2>Quelques optimisations dans Spark</h2>
  <ul>
    <li><code>PushDownPredicate</code></li>
    <li><code>ColumnPruning</code></li>
    <li><code>CombineFilters</code></li>
  </ul>
</section>
<section>
  <img src="./images/catalystExample.png" style="background:none; border:none; box-shadow:none;"/>
</section>
<section>
  <h2>Une structure en arbre idéale pour agencer et optimiser le plan d'exécution</h2>
</section>
<section>
  <h2>Définition d'un <em>Trait</em></h2>
  <pre class="bigger-code scala"><code data-trim data-noescape>
trait TreeNode[Type <: TreeNode[Type]] {
                       self: Type =>

  def children: Seq[Type]

  def copyWithNewChildren(children: Seq[Type]): Type = ???

  def mapChildren(f: Type => Type): Type = {
    copyWithNewChildren(children.map(f))
  }

  def transform(pf: PartialFunction[Type, Type]): Type = {
    pf.applyOrElse(this, identity[Type])
      .mapChildren(_.transformDown(pf))
  }

}
  </code></pre>
</section>
<section>
  <h2>Exemple d'optimisation</h2>
  <pre class="bigger-code scala"><code data-trim data-noescape>
object CombineFilters extends Rule[LogicalPlan] {
  def apply(plan: LogicalPlan): LogicalPlan = plan transform {
    case Filter(exp1, Filter(exp2, grandChild)) =>
          Filter(And(exp1, exp2), grandChild)
  }
}
  </code></pre>
</section>
<section>
  <h2>Ok mais où est la génération de code ?</h2>
</section>
<section>
  <h2>Volcano model</h2>
  <div>1 opérateur = 1 classe</div>
  <div>1 opérateur = 1 appel à une fonction virtuelle</div>
  <div>On perd le pipelining</div>
</section>
<section>
  <div>En fait, un bout de code écrit à la main va beaucoup plus vite</div>
  <pre class="bigger-code java"><code data-trim data-noescape>
while(rows.hasNext()) {
  Row currentRow = rows.next();
  if(currentRow.get("age") > 18)
    return currentRow.get("name");
}
  </code></pre>
  <pre class="bigger-code scala"><code data-trim data-noescape>
    Project("name",
      Filter(
        Expression("age", greaterThan(18)),
        CsvScan("people.csv")
      )
    )
  </code></pre>
</section>
<section>
  <h2>Spark introduit le <code>WholeStageCodegen</code></h2>
  <div>1 classe = n opérateurs</div>
</section>
<section>
  <aside class="notes">Schéma avec plusieurs opérateurs regroupés en 1 seul</aside>
</section>
<section>
  <h3>Comment avoir une seule classe qui contient les opérations de plusieurs classes sans les référencer ?</h3>
  <div class="fragment">La génération de code !!</div>
  <aside class="notes">
    La génération de code nous permet d'avoir les performances d'un langage impératif avec la fléxibilité d'un langage orienté objet : mindblow !!
  </aside>
</section>
