<section data-background-class="mandy">
  <h2>Aperçu d'<em>Apache Spark</em></h2>
  <aside class="notes">
Pas de panique, nous allons d'abord faire quelques rappels sur ce qu'est Apache Spark <br />
Qui dans la salle connait Apache Spark et l'utilise en production ?
  </aside>
</section>

<section data-speaker="adrien">
  <img src="./images/apache-spark-logo.svg" />
  <img src="./images/spark.png" />
  <aside class="notes">
Pour les autres, c'est pas grave, on va y aller doucement. <br />
Apache Spark est un framework de calcul distribué écrit en Scala <br />
C'est la société Databricks qui gère le développement de l'outil <br />
L'idée, c'est de définir des sources de données en entrée et un stockage en sortie et l'on défini des transformations entre les deux<br />
On peut voir ça comme une espèce d'ETL distribué <br />
  </aside>
</section>

<section data-speaker="adrien">
  <h3>En résumé, <em>Apache Spark</em>&nbsp;:</h3>
  <ul>
    <li>Est un <strong>framework de calcul distribué</strong> <em>In Memory</em>&nbsp;;</li>
    <li>Dont certains noeuds <strong>orchestrent</strong> et d'autres <strong>exécutent</strong>&nbsp;;</li>
    <li>Qui lance des <strong>Jobs</strong> qui sont <strong>tolérants à la panne</strong>&nbsp;;</li>
    <li>Se base sur des <strong>abstractions</strong> de niveaux différents. </li>
  </ul>
  <aside class="notes">
Spark est résilient : si l'un des noeuds du cluster tombe, on peut s'en sortir (même si certains calculs intermédiaires vont être fait à nouveau) <br />
Les abstractions ne sont pas des évolutions mais bel et bien des niveaux d'abstractions différents <br />
  </aside>
</section>

<section>
  <h3>Jeu de données&nbsp;:</h3>
  <div>
    <div style="font-size: 0.75em;">
      <p><code>employees</code></p>
      <table>
        <thead>
          <tr>
            <th>firstName</th>
            <th>lastName</th>
            <th>companyId</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Adrien</td>
            <td>Besnard</td>
            <td>1</td>
          </tr>
          <tr>
            <td>Marc</td>
            <td>Alonso</td>
            <td>1</td>
          </tr>
          <tr>
            <td>Mark</td>
            <td>Zuckerberg</td>
            <td>2</td>
          </tr>
        </tbody>
      </table>
    </div>
    <div style="margin-top: 1em; font-size: 0.75em;">
      <p><code>companies</code></p>
      <table>
        <thead>
          <tr>
            <th>name</th>
            <th>id</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>OCTO</td>
            <td>1</td>
          </tr>
          <tr>
            <td>Facebook</td>
            <td>2</td>
          </tr>
        </tbody>
      </table>
    </div>
  </div>
  <aside class="notes">
Petit point sur ce jeu de données : tout ce que l'on va vous expliquer dans les slides suivants, on va essayer de l'illustrer en se basant sur ce jeu de données <br />
Ne le perdez pas de vue !
  </aside>
</section>

<section>
  <h4>Les <code>RDD</code></h4>
  <div>
    <p>Les <code>RDD[_]</code>&nbsp;: un <code>Iterator[_]</code> distribué et résilient. </p>
    <pre class="bigger-code scala fragment"><code data-trim data-noescape>
val employeesRDD = sparkContext.parallelize(Employee.sample)
val companiesRDD = sparkContext.parallelize(Company.sample)

val employeesByCompanyIdRDD = employeesRDD
  .map({ e => (e.companyId, e) })

val companiesByIdRDD = companiesRDD
  .map({ c => (c.id, c) })

val octoEmployees = employeesByCompanyIdRDD.join(companiesByIdRDD)
  .map({ case (_, (e, c)) => (e, c) })
  .filter({ case (e, c) => c.name == "OCTO" })
  .map({ case (e, c) => (e.firstName, e.lastName) })

octoEmployees.collect()
  .foreach(println)
    </code></pre>
    <pre class="bigger-code scala fragment"><code data-trim data-noescape>
// (Marc,Alonso)
// (Adrien,Besnard)
    </code></pre>
  </div>
  <aside class="notes">
Le RDD, c'est un peu l'assembleur de Spark. C'est litérallement un iterateur distribué <br />
Pour ceux qui connaissent un peu les streams en Java ou juste le framework collection de Scala, Spark fourni les mêmes méthodes pour transformer de la données. <br />
Un petit point qui n'est pas très important pour la suite, mais qu'il faut retenir, c'est que Spark défini les transformations de manière lazy <br />
C'est à dire que ces transformations ne sont réalisées qu'une fois qu'une action déclenche le traitement (ici, c'est le foreach)
  </aside>
</section>

<section>
  <h4>Les <code>DataFrames</code></h4>
  <div>
    <p>Les <code>DataFrame</code> qui fournissent un DSL de manipulation des données structurées.</p>
    <pre class="bigger-code scala fragment"><code data-trim data-noescape>
val employeesDF = sparkSession.createDataFrame(Employee.sample)
val companiesDF = sparkSession.createDataFrame(Company.sample)

val resultDF = companiesDF
  .join(
    employeesDF,
    $"companyId" === $"id"
  )
.select($"firstName", $"lastName")
.where($"name" === "OCTO")
    </code></pre>
    <pre class="bigger-code scala fragment"><code data-trim data-noescape>
// +---------+--------+
// |firstName|lastName|
// +---------+--------+
// |   Adrien| Besnard|
// |     Marc|  Alonso|
// +---------+--------+
    </code></pre>
  </div>
  <aside class="notes">
Les Dataframes sont une abstraction de plus haut niveau, mais sous le capot, elles s'appuient aussi sur les RDD <br />
On a vu que le code précédement était un peu compliqué, mais on a ici un DSL qui permet de manipuler des données structurées (en gros, des tables avec un schéma qui ont des colonnes) <br />
On dirait presque du SQL...
  </aside>
</section>

<section>
  <h4>Le SQL</h4>
  <div>
    <p>Le SQL qui permet de réaliser des requêtes homogènes sur des sources de données hétérogènes.</p>
    <pre class="bigger-code scala fragment"><code data-trim data-noescape>
employeesDF.createOrReplaceTempView("employees")
companiesDF.createOrReplaceTempView("companies")

val resultDF = sparkSession.sql(
  """
    |SELECT firstName, lastName
    |  FROM employees
    |  JOIN companies
    |    ON companyId = id
    | WHERE name = 'OCTO'
  """.stripMargin)
resultDF.show()
    </code></pre>
    <pre class="bigger-code scala fragment"><code data-trim data-noescape>
// +---------+--------+
// |firstName|lastName|
// +---------+--------+
// |   Adrien| Besnard|
// |     Marc|  Alonso|
// +---------+--------+
    </code></pre>
  </div>
  <aside class="notes">
Le SQL est une autre abstraction qui est du même niveau que les Dataframes (sous le capot, une requête SQL et une DataFrame sont équivalente) <br />
Cette abstraction va nous permettre de faire des requêtes SQL homogènes (de par le langage standard) sur des sources de données hétérogènes <br />
Pour l'exemple, on a ici des données qui viennent d'une séquence statique, mais elles pourraient venir d'une table Hive d'un côté et d'un fichier CSV de l'autre
  </aside>
</section>

<section data-speaker="adrien">
  <h3>Le projet <em>Tungsten</em>&nbsp;:</h3>
  <ul>
    <li>Initié à partir d'<em>Apache Spark</em> 1.4&nbsp;;</li>
    <li>Ayant pour objectif une <strong>amélioration des performances</strong> sans changement de l'API&nbsp;;</li>
    <li><blockquote>CPU is the new bottleneck</blockquote>&nbsp;;</li>
    <li>Depuis <em>Apache Spark</em> 2.0, <strong>génération de code</strong> à la volée. </li>
  </ul>
  <aside class="notes">
Suite aux bench de Databricks : CPU is the bottelneck
  </aside>
</section>

<section data-speaker="adrien">
  <img src="./images/wait-a-minute.jpg" />
  <aside class="notes">
Qu'est ce que ça veut dire génération de code à la volée ?
  </aside>
</section>
