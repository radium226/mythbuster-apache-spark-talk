<section data-background-class="mandy">
  <h2>Aperçu d'<em>Apache Spark</em></h2>
</section>

<section data-speaker="adrien">
  <img src="./images/apache-spark-logo.svg" />
  <aside class="notes">
Disclaimer : Apache Spark, c'est pas grave et on va y aller doucement.
Apache Spark, c'est créé par DataBricks
  </aside>
</section>

<section data-speaker="adrien">
  <h3>En résumé, <em>Apache Spark</em>&nbsp;:</h3>
  <ul>
    <li>Est un <strong>framework de calcul distribué</strong> <em>In Memory</em>&nbsp;;</li>
    <li>Se base sur un <em>Cluster</em> avec certains noeuds qui <strong>orchestrent</strong> et d'autres <strong>exécutent</strong>&nbsp;;</li>
    <li>Qui lance des <strong>Jobs</strong> qui sont <strong>tolérants à la panne</strong>&nbsp;;</li>
    <li>Se base sur des <strong>abstractions</strong> de plusieurs niveaux&nbsp;;</li>
    <li>Permet l'<strong>utilisation du SQL</strong> sur des sources de données hétérogènes. </li>
  </ul>
  <aside class="notes">
    Schéma d'un ETL distribué
  </aside>
</section>

<section>
  <h3>Les <code>RDD</code></h3>
  <div>
    <p>Les <code>RDD[_]</code>&nbsp;: un <code>Iterator[_]</code> distribué et résilient. </p>
    <pre class="bigger-code scala fragment"><code data-trim data-noescape>
val speakers: Seq[Speaker] = ???
val speakersRDD: RDD[Speaker] = sparkContext
  .parallelize(speakers)
speakersRDD
  .filter({ speaker =>
    Seq("Adrien", "Marc").contains(speaker.firstName)
  })
  .map(_.lastName)
  .saveAsTextFile("MythBuster.txt")
    </code></pre>
  </div>
</section>

<section>
  <h3>Les <code>DataFrames</code></h3>
  <div>
    <p>Les <code>DataFrame</code>&nbsp; qui permettent d'avoir un DSL équivalent au SQL pour manipuler les données structurées.</p>
    <pre class="bigger-code scala fragment"><code data-trim data-noescape>
val speakersDF = speakersRDD.toDF()
speakersDF
  .select($"firstName")
  .where($"lastName" === "Alonso")
  .show()
    </code></pre>
  </div>
</section>

<section>
  <h3>Le SQL</h3>
  <div>
    <p>Le SQL qui permet de se rapprocher des SGBDR classiques</p>
    <pre class="bigger-code scala fragment"><code data-trim data-noescape>
speakersDF.createOrReplaceTempView("persons")
spark.sql("SELECT * FROM persons WHERE lastName = 'Alonso'").show()
    </code></pre>
  </div>
</section>

<section data-speaker="adrien">
  <h3>Le projet <em>Tungsten</em></h3>
  <ul>
    <li>Initié à partir d'<em>Apache Spark</em> 1.4&nbsp;;</li>
    <li>Ayant pour objectif une <strong>amélioration des performances</strong> sans changement de l'API&nbsp;;</li>
    <li><blockquote>CPU is the new bottlneck</blockquote>&nbsp;;</li>
    <li>Depuis <em>Apache Spark</em> 2.0, <strong>génération de code</strong> à la volée. </li>
  </ul>
  <aside class="notes">
    Schéma avec SELECT<br />
    Suite aux bench de Databricks : CPU is the bottelneck
  </aside>
</section>

<section data-speaker="adrien">
  <img src="./images/wait-a-minute.jpg" />
  <aside class="notes">
    Image WTF ? <br />
    Pour bien comprendre, on farfouille dans Spark <br />
    Qu'est ce que ça veut dire génération de code à la volée ?
  </aside>
</section>
