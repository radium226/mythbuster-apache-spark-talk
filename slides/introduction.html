<section data-background-class="mandy">
  <h2>Aperçu d'<em>Apache Spark</em></h2>
  <aside class="notes">
Avant de rentrer dans le coeur du sujet, remettons-nous tous à niveau
  </aside>
</section>

<section data-speaker="adrien">
  <img src="./images/apache-spark-logo.svg" />
  <aside class="notes">
Disclaimer : Apache Spark, c'est pas grave et on va y aller doucement.
Pour ceux qui ne connaissent pas, Spark c'est un framework de calcul distribué : c'est écrit en Scala,
on désinge des sources de données en entrée, un stockage en sortie,
  et on définit des transformations entre les deux.
Il existe plusieurs niveaux d'abstraction pour décrire les transformations :
      - RDD : impératif
      - Dataframe/SQL : déclaratif
La donnée est récupéré
Apache Spark, c'est créé par DataBricks
  </aside>
</section>

<section data-speaker="adrien">
  <h3>En résumé, <em>Apache Spark</em>&nbsp;:</h3>
  <ul>
    <li>Est un <strong>framework de calcul distribué</strong> <em>In Memory</em>&nbsp;;</li>
    <li>Dont certains noeuds <strong>orchestrent</strong> et d'autres <strong>exécutent</strong>&nbsp;;</li>
    <li>Qui lance des <strong>Jobs</strong> qui sont <strong>tolérants à la panne</strong>&nbsp;;</li>
    <li>Se base sur des <strong>abstractions</strong> de niveaux différents. </li>
  </ul>
  <aside class="notes">
    Schéma d'un ETL distribué
  </aside>
</section>

<section>
  <h3>Jeu de données&nbsp;:</h3>
  <div>
    <div style="font-size: 0.75em;">
      <p><code>employees</code></p>
      <table>
        <thead>
          <tr>
            <th>firstName</th>
            <th>lastName</th>
            <th>companyId</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Adrien</td>
            <td>Besnard</td>
            <td>1</td>
          </tr>
          <tr>
            <td>Marc</td>
            <td>Alonso</td>
            <td>1</td>
          </tr>
          <tr>
            <td>Mark</td>
            <td>Zuckerberg</td>
            <td>2</td>
          </tr>
        </tbody>
      </table>
    </div>
    <div style="margin-top: 1em; font-size: 0.75em;">
      <p><code>companies</code></p>
      <table>
        <thead>
          <tr>
            <th>name</th>
            <th>id</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>OCTO</td>
            <td>1</td>
          </tr>
          <tr>
            <td>Facebook</td>
            <td>2</td>
          </tr>
        </tbody>
      </table>
    </div>
  </div>
</section>

<section>
  <h4>Les <code>RDD</code></h4>
  <div>
    <p>Les <code>RDD[_]</code>&nbsp;: un <code>Iterator[_]</code> distribué et résilient. </p>
    <pre class="bigger-code scala fragment"><code data-trim data-noescape>
val employeesRDD = sparkContext.parallelize(Employee.sample)
val companiesRDD = sparkContext.parallelize(Company.sample)

val employeesByCompanyIdRDD = employeesRDD
  .map({ e => (e.companyId, e) })

val companiesByIdRDD = companiesRDD
  .map({ c => (c.id, c) })

val octoEmployees = employeesByCompanyIdRDD.join(companiesByIdRDD)
  .map({ case (_, (e, c)) => (e, c) })
  .filter({ case (e, c) => c.name == "OCTO" })
  .map({ case (e, c) => (e.firstName, e.lastName) })

octoEmployees.collect()
  .foreach(println)
    </code></pre>
    <pre class="bigger-code scala fragment"><code data-trim data-noescape>
// (Marc,Alonso)
// (Adrien,Besnard)
    </code></pre>
  </div>
</section>

<section>
  <h4>Les <code>DataFrames</code></h4>
  <div>
    <p>Les <code>DataFrame</code> qui fournissent un DSL de manipulation des données structurées.</p>
    <pre class="bigger-code scala fragment"><code data-trim data-noescape>
val employeesDF = sparkSession.createDataFrame(Employee.sample)
val companiesDF = sparkSession.createDataFrame(Company.sample)

val resultDF = companiesDF
  .join(
    employeesDF,
    $"companyId" === $"id"
  )
.select($"firstName", $"lastName")
.where($"name" === "OCTO")
    </code></pre>
    <pre class="bigger-code scala fragment"><code data-trim data-noescape>
// +---------+--------+
// |firstName|lastName|
// +---------+--------+
// |   Adrien| Besnard|
// |     Marc|  Alonso|
// +---------+--------+
    </code></pre>
  </div>
</section>

<section>
  <h4>Le SQL</h4>
  <div>
    <p>Le SQL qui permet de réaliser des requêtes homogènes sur des sources de données hétérogènes.</p>
    <pre class="bigger-code scala fragment"><code data-trim data-noescape>
employeesDF.createOrReplaceTempView("employees")
companiesDF.createOrReplaceTempView("companies")

val resultDF = sparkSession.sql(
  """
    |SELECT firstName, lastName
    |  FROM employees
    |  JOIN companies
    |    ON companyId = id
    | WHERE name = 'OCTO'
  """.stripMargin)
resultDF.show()
    </code></pre>
    <pre class="bigger-code scala fragment"><code data-trim data-noescape>
// +---------+--------+
// |firstName|lastName|
// +---------+--------+
// |   Adrien| Besnard|
// |     Marc|  Alonso|
// +---------+--------+
    </code></pre>
  </div>
</section>

<section data-speaker="adrien">
  <h3>Le projet <em>Tungsten</em>&nbsp;:</h3>
  <ul>
    <li>Initié à partir d'<em>Apache Spark</em> 1.4&nbsp;;</li>
    <li>Ayant pour objectif une <strong>amélioration des performances</strong> sans changement de l'API&nbsp;;</li>
    <li><blockquote>CPU is the new bottlneck</blockquote>&nbsp;;</li>
    <li>Depuis <em>Apache Spark</em> 2.0, <strong>génération de code</strong> à la volée. </li>
  </ul>
  <aside class="notes">
    Schéma avec SELECT<br />
    Suite aux bench de Databricks : CPU is the bottelneck
  </aside>
</section>

<section data-speaker="adrien">
  <img src="./images/wait-a-minute.jpg" />
  <aside class="notes">
    Image WTF ? <br />
    Pour bien comprendre, on farfouille dans Spark <br />
    Qu'est ce que ça veut dire génération de code à la volée ?
  </aside>
</section>
