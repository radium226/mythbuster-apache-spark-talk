<section data-background-class="mandy">
  <h2>Aperçu d'<em>Apache Spark</em></h2>
  <aside class="notes">
Avant de rentrer dans le coeur du sujet, remettons-nous tous à niveau
  </aside>
</section>

<section data-speaker="adrien">
  <img src="./images/apache-spark-logo.svg" />
  <aside class="notes">
Disclaimer : Apache Spark, c'est pas grave et on va y aller doucement.
Pour ceux qui ne connaissent pas, Spark c'est un framework de calcul distribué : c'est écrit en Scala,
on désinge des sources de données en entrée, un stockage en sortie,
  et on définit des transformations entre les deux.
Il existe plusieurs niveaux d'abstraction pour décrire les transformations :
      - RDD : impératif
      - Dataframe/SQL : déclaratif
La donnée est récupéré
Apache Spark, c'est créé par DataBricks
  </aside>
</section>

<section data-speaker="adrien">
  <h3>En résumé, <em>Apache Spark</em>&nbsp;:</h3>
  <ul>
    <li>Est un <strong>framework de calcul distribué</strong> <em>In Memory</em>&nbsp;;</li>
    <li>Dont certains noeuds <strong>orchestrent</strong> et d'autres <strong>exécutent</strong>&nbsp;;</li>
    <li>Qui lance des <strong>Jobs</strong> qui sont <strong>tolérants à la panne</strong>&nbsp;;</li>
    <li>Se base sur des <strong>abstractions</strong> de niveaux différents. </li>
  </ul>
  <aside class="notes">
    Schéma d'un ETL distribué
  </aside>
</section>

<section>
  <h3>Les <code>RDD</code></h3>
  <div>
    <p>Les <code>RDD[_]</code>&nbsp;: un <code>Iterator[_]</code> distribué et résilient. </p>
    <pre class="bigger-code scala fragment"><code data-trim data-noescape>
val speakers: Seq[Speaker] = ???
val speakersRDD: RDD[Speaker] = sparkContext
  .parallelize(speakers)
speakersRDD
  .filter({ speaker =>
    Seq("Adrien", "Marc").contains(speaker.firstName)
  })
  .map(_.lastName)
  .saveAsTextFile("MythBuster.txt")
    </code></pre>
  </div>
</section>

<section>
  <h3>Les <code>DataFrames</code></h3>
  <div>
    <p>Les <code>DataFrame</code> qui fournissent un DSL de manipulation des données structurées.</p>
    <pre class="bigger-code scala fragment"><code data-trim data-noescape>
val speakersDF = speakersRDD.toDF()
speakersDF
  .select($"firstName")
  .where($"lastName" === "Alonso")
  .show()
    </code></pre>
  </div>
</section>

<section>
  <h3>Le SQL</h3>
  <div>
    <p>Le SQL qui permet de réaliser des requêtes homogènes sur des sources de données hétérogènes.</p>
    <pre class="bigger-code scala fragment"><code data-trim data-noescape>
speakersDF.createOrReplaceTempView("persons")
spark.sql("SELECT * FROM persons WHERE lastName = 'Alonso'").show()
    </code></pre>
  </div>
</section>

<section data-speaker="adrien">
  <h3>Le projet <em>Tungsten</em></h3>
  <ul>
    <li>Initié à partir d'<em>Apache Spark</em> 1.4&nbsp;;</li>
    <li>Ayant pour objectif une <strong>amélioration des performances</strong> sans changement de l'API&nbsp;;</li>
    <li><blockquote>CPU is the new bottlneck</blockquote>&nbsp;;</li>
    <li>Depuis <em>Apache Spark</em> 2.0, <strong>génération de code</strong> à la volée. </li>
  </ul>
  <aside class="notes">
    Schéma avec SELECT<br />
    Suite aux bench de Databricks : CPU is the bottelneck
  </aside>
</section>

<section data-speaker="adrien">
  <img src="./images/wait-a-minute.jpg" />
  <aside class="notes">
    Image WTF ? <br />
    Pour bien comprendre, on farfouille dans Spark <br />
    Qu'est ce que ça veut dire génération de code à la volée ?
  </aside>
</section>
