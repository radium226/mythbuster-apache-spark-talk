<section data-background-class="saffron-mango" data-talker="adrien">
  <h2>Plan d'exécution logique</h2>
  <aside class="notes">
On va voir qu'en fait, l'execution se fait en deux étapes <br />
La première, c'est de transformer notre AST dans un Plan d'execution logique
  </aside>
</section>

<section>
  <h3>Deux contexts distincts&hellip;</h3>
  <img src="./images/query-planner.png" />
  <pre class="bigger-code scala fragment"><code data-trim data-noescape>
def plan: AST => LogicalPlan = ??? // On a quasiement
                                   // du un pour un.
  </code></pre>
  <aside class="notes">
On va passer sous silence l'implémentation de la fonction plan qui permet de passer d'un AST à un plan logique, parce qu'en fait c'est quasiement du 1 pour 1 <br />
L'idée, c'est juste de changer de contexte / d'environnement <br />
L'AST correspond au langage tandis que le plan d'exection logique va être dans l'execution
  </aside>
</section>

<section data-speaker="adrien">
  <h3>De quoi parle-t-on&nbsp;?</h3>
  <p>L'idée est de décrire <strong>les différentes étapes</strong> nécessaire à l'évaluation l'<em>AST</em>.</p>
  <pre class="bigger-code scala fragment"><code data-trim data-noescape>
sealed trait LogicalPlan {

  def provideColumns: Seq[ColumnName]

}
  </code></pre>
  <aside class="notes">
Le plan d'execution logique va juste correspondre aux différentes étapes par lesquelles il faut passer pour executer la requête <br />
On parle ici de description, la requête n'est pas executée à ce niveau là. <br />
Le plan d'execution logique, c'est juste une vision algébrique de ta requête (c'est vraiment un plan) <br />
La seule différence avec l'AST pour le moment, c'est que l'on va devoir définir des implémentation du trait LogicalPlan qui soit capable de nous fournir les noms de colonnes qu'il fournisse <br />
C'est interessant de noter que Spark (et dans les bases de données en général), le fait de fournir les colonnes fournies pour une table source... Correspond à la définition du schéma!
  </aside>
</section>

<section data-speaker="adrien">
  <pre class="bigger-code scala"><code data-trim data-noescape>
case class Join(
  leftChild: LogicalPlan, rightChild: LogicalPlan,
  predicate: Expression
) extends LogicalPlan {

  override def produceColumns = {
    leftChild.provideColumns ++ rightChild.provideColumns
  }

}
  </code></pre>
  <pre class="bigger-code scala fragment"><code data-trim data-noescape>
case class Projection(
  child: LogicalPlan, expression: Seq[Expression]
) extends LogicalPlan {
  // ...
}
  </code></pre>
  <pre class="bigger-code scala fragment"><code data-trim data-noescape>
case class Filter(
  child: LogicalPlan,
  expression: Expression
) extends LogicalPlan {
  // ...
}
  </code></pre>
  <p class="fragment">Purement <strong>déclaratif</strong>, toujours pas d'execution !</p>
  <aside class="notes">
Vous avez tous entendu parlé de HashJoin ou de NestedLoopJoin... Bah c'est pas ici. Encore une fois, ici, ce n'est que délcaratif <br />
Spark permet de faire des requêtes SQL sur des sources de données hétérogènes... Mais ici, on aura des Scan, mais de JdbcScan ou de HiveScan... <br />
On a ici le "ce que l'on veut faire", mais pas le "comment on va le faire"
  </aside>
</section>

<section>
  <h4>Exemple&nbsp;:</h4>
  <img src="./images/volcano-model-logical-plan.png" />
</section>

<section>
  <h4>Un peu d'algèbre&nbsp;:</h4>
  <p>Cela reprend le concept des opérateurs de l'algèbre relationnelle.</p>
  <img src="./images/logical-plan-vs-operateur-algebrique.png" />
</section>

<section data-transition="fade-out">
  <h5>Chaque opérateur représente une action&nbsp;:</h5>
  <img src="./images/optimisation-logique_1.png" />
</section>

<section data-transition="fade-out fade-in">
  <h5>Chaque opérateur représente une action&nbsp;:</h5>
  <img src="./images/optimisation-logique_2.png" />
</section>

<section data-transition="fade-out fade-in">
  <h5>Chaque opérateur représente une action&nbsp;:</h5>
  <img src="./images/optimisation-logique_3.png" />
</section>

<section data-transition="fade-in">
  <h5>Chaque opérateur représente une action&nbsp;:</h5>
  <img src="./images/optimisation-logique_4.png" />
</section>

<section>
  <h3>Les optimisations&nbsp;:</h3>
  <ul>
    <li>L'objectif est de <strong>réduire le temps d'execution</strong> sans changer le résultat&nbsp;;</li>
    <li>On utilise pour cela un certain nombre de règles algébriques&nbsp;;</li>
    <li>Ces règles appliquées au sein d'un <em>Rule Based Optimizer</em>&nbsp;;</li>
  </ul>
  <pre class="bigger-code scala fragment"><code data-trim data-noescape>
def optimize: LogicalPlan => LogicalPlan = ???
  </code></pre>
  <aside class="notes">
Avant de passer au plan d'execution physique (qui va réellement nous permette d'executer la requête), il y a une étape d'optimisation <br />
L'idée, c'est juste d'appliquer des règles qui vont nous permettre in fine d'executer la requête <br />
Ce sont des règles purement logique (qui ne s'appuie pas sur la données en tant que tel, comme la volumétrie des tables en jeu, par exemple)
  </aside>
</section>

<section>
  <h4>Encore un peu d'algèbre&nbsp;:</h4>
  <p>Les lois algébriques prouvent que le resultat de la requête ne sera pas changé après optimisation. </p>
  <img src="./images/algebrical-rules.png" />
  <aside class="notes">
Vu que le plan d'execution logique s'appuie sur une description algébrique des transformations à réaliser pour executer une requête <br />
Il est possible d'appliquer des lois algébriques qui vont nous aider à optimiser la requête
  </aside>
</section>

<section>
  <h4>Exemple&nbsp;:</h4>
  <p>Les schémas qui vont bien</p>
</section>

<section>
  <h4>Cas du <code>PushDownFilter</code>&nbsp;:</h4>
  <p>L'objectif est de passer l'étape <code>Filter</code> au plus tôt. </p>
  <pre class="bigger-code scala"><code data-trim data-noescape>
object PushDownFilter extends Rule[LogicalPlan] {

  override def apply(logicalPlan: LogicalPlan): LogicalPlan = {
    logicalPlan transformDown {
      case Filter(
        Join(leftChild, rightChild, joinExpression),
        filterExpression
      ) if leftChild.fullfillColumnsFor(filterExpression) =>
        Join(
          Filter(leftChild, filterExpression),
          rightChild, joinExpression
      )

       // ...
    }
  }

}
  </code></pre>
</section>

<section>
  <h3>Du côté d'<em>Apache Spark</em>&nbsp;?</h3>
  <div>
    <div class="left-column">
      <pre class="bigger-code scala"><code data-trim data-noescape>
// Operator push down
PushProjectionThroughUnion,
ReorderJoin(conf),
EliminateOuterJoin(conf),
PushPredicateThroughJoin,
PushDownPredicate,
LimitPushDown(conf),
ColumnPruning,
InferFiltersFromConstraints(conf),
// Operator combine
CollapseRepartition,
CollapseProject,
CollapseWindow,
CombineFilters,
CombineLimits,
CombineUnions,
// Constant folding
NullPropagation(conf),
FoldablePropagation,
OptimizeIn(conf),
ConstantFolding,
// ...
      </code></pre>
    </div>
    <div class="right-column">
      <ul>
        <li>Les optimisations font parties du moteur <em>Catalyst</em>&nbsp;;</li>
        <li>Des règles supplémentaires ajoutées à chaque nouvelle version&nbsp;;</li>
        <li>Utilisation du trait <code>TreeNode</code> qui permet d'avoir la méthode <code>transformDown</code>. </li>
      </ul>
    </div>
  </div>
</section>

<section>
  <pre class="bigger-code"><code data-trim data-noescape>
sparkSession.sql(
  """
    |SELECT firstName, lastName
    |  FROM employees
    |  JOIN companies
    |    ON companyId = id
    | WHERE name = 'OCTO'
  """.stripMargin).explain()
    </code></pre>
    <pre class="bigger-code fragment"><code data-trim data-noescape>
== Logical Plan ==
'Project ['firstName, 'lastName]
+- 'Filter ('name = OCTO)
   +- 'Join Inner, ('companyId = 'id)
      :- 'UnresolvedRelation `employees`
      +- 'UnresolvedRelation `companies`
  </code></pre>
  <pre class="bigger-code fragment"><code data-trim data-noescape>
== Optimized Logical Plan ==
Project [firstName#39, lastName#40]
+- Join Inner, (companyId#41 = id#47)
  :- LocalRelation [firstName#39, lastName#40, companyId#41]
  +- Project [id#47]
     +- Filter (isnotnull(name#48) && (name#48 = OCTO))
        +- LocalRelation [id#47, name#48]
  </code></pre>
</section>

<section data-speaker="marc">
  <h3>Ce qu'il faut retenir&nbsp;:</h3>
  <ul>
    <li>Le plan d'exécution logique permet de définir les étapes par lequel le moteur doit passer pour exécuter une requête SQL&nbsp;;</li>
    <li>Il peut être optimisé en s'appuyant sur des règles algébriques&nbsp;;</li>
    <li>Il est purement déclaratif. </li>
  </ul>
  <p><span class="fragment">Maintenant que l'on sait ce que l'on va exécuter&hellip; </span><span class="fragment">Exécutons-le&nbsp;!</span></p>
</section>
