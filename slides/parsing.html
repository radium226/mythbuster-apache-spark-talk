<section data-background-class="scooter">
  <h2>Parsing d'une requête SQL</h2>
</section>

<section data-speaker="marc">
  <h3>Deux étapes&nbsp;:</h3>
  <img src="./images/lexing_parsing.png" />
  <aside class="notes">
    Résumé de tout sous forme de schéma
  </aside>
</section>

<section data-speaker="marc">
  <h4>L'analyse lexicale</h4>
  <img src="./images/lexing_2.png" />
  <aside class="notes">
    Schéma avec un exemple de requête SQL
  </aside>
</section>

<section data-speaker="marc">
  <pre class="bigger-code scala"><code data-trim data-noescape>
sealed trait Token

case class Select() extends Token

case class From() extends Token

// ...
  </code></pre>
  <pre class="bigger-code scala fragment"><code data-trim data-noescape>
def lexer: SQL => Seq[Token] = ???
  </code></pre>
</section>

<section data-speaker="marc">
  <h4>L'analyse syntaxique</h4>
  <img src="./images/parsing_2.png" />
  <aside class="notes">
    Pareil, un schéma depuis les token vers l'AST
  </aside>
</section>

<section data-speaker="marc">
  <pre class="bigger-code scala"><code data-trim data-noescape>
trait AST

case class Table(name: String) extends AST

case class Select(
  projections: Seq[Expression],
  filter: Option[Expression],
  relations: Seq[Table]
) extends AST
  </code></pre>
  <pre class="bigger-code scala fragment"><code data-trim data-noescape>
trait Expression extends AST {

  def evaluate(row: Row): AnyRef

  def needColumns: Seq[ColumnName]

}

// ...
  </code></pre>

  <pre class="bigger-code scala fragment"><code data-trim data-noescape>
def parser: Seq[Token] => AST
  </code></pre>
  <aside class="notes">
    Le parser permet de donner un sens aux tokens : de hiérarchiser les tokens, et contextualiser tout ça. <br />
    N'importe quel language utilise ça (y a un AST pour Java, pour Scala, etc.) y a rien de révolutionnaire et ça n'est pas du tout propre à Spark
  </aside>
</section>

<section data-speaker="marc">
  <h3>Implémentations&nbsp;:</h3>
  <div>
    <div class="left-column">
      <img src="./images/scala-logo.png" />
      <h4 style="text-align: center;">Parser Combinators</h4>
      <pre class="bigger-code scala"><code data-trim data-noescape>
object Parser extends Parsers {

  override type Elem = l.Token

  def literal: Parser[e.Literal] =
    { number | text }

  def from = l.From()
               ~> tables

// ...
      </code></pre>
    </div>
    <div class="right-column fragment">
      <img style="width: 50%; height: 50%;" src="./images/antlr-logo.png" />
      <pre class="bigger-code"><code data-trim data-noescape>
statement
  : query
  | USE db=identifier
  | CREATE DATABASE
    (IF NOT EXISTS)? identifier
    (COMMENT comment=STRING)?
      locationSpec?
    (WITH DBPROPERTIES
      tablePropertyList)
      </code></pre>
    </div>
  </div>
  <aside class="notes">
    Logos de Parser Combinator et ANTLR <br />
    Pour nous, les Parser Combinator s'était suffisant, mais ANTLR c'est plus sérieux pour des langages beaucoup plus complexes (type SQL ANSI, etc.)
  </aside>
</section>

<section data-speaker="marc">
  <h3>Ce qu'il faut retenir&nbsp;:</h3>
  <ul>
    <li>Une étape d'analyse lexicale : <code>String => Seq[Token]</code></li>
    <li>Une étape d'analyse syntaxique : <code>Seq[Token] => AST</code></li>
    <li>L'<em>AST</em> est une structure qui a du sens en termes de types : exploitable par le code</li>
    <li><strong>ANTLR est un standard</strong> pour des langages complexes, et c'est ce qu'utilise <em>Apache Spark</em>. </li>
  </ul>
  <p><span class="fragment">Notre requête SQL étant parsée&hellip; </span><span class="fragment">Il n'y plus qu'à l'executer&nbsp;!</span></p>
</section>
