<section data-background-class="scooter">
  <h2>Parsing d'une requête SQL</h2>
</section>

<section data-speaker="marc">
  <h3>Deux étapes&nbsp;:</h3>
  <img src="./images/lexing_parsing.png" />
  <aside class="notes">
Le parsing d'une requête se fait en deux étapes que nous allons voir <br />
Le lexing et le parsing en tant que tel
  </aside>
</section>

<section data-speaker="marc">
  <h4>L'analyse lexicale</h4>
  <img src="./images/lexing_2.png" />
  <aside class="notes">
Le lexing (aussi appeler tokeninzation ou analyse lexical) correspond juste à l'extraction depuis notre String de tokens <br />
Les tokens sont, en gros : soit des mots clés (SELECT, etc.) soit des literaux (les parties dynamique de la requête, comme les noms de tables, etc.)
  </aside>
</section>

<section data-speaker="marc">
  <pre class="bigger-code scala"><code data-trim data-noescape>
sealed trait Token

case class Select() extends Token

case class From() extends Token

// ...
  </code></pre>
  <pre class="bigger-code scala fragment"><code data-trim data-noescape>
def tokenize: String => Seq[Token] = ???
  </code></pre>
  <aside class="notes">
Au niveau du code, il suffit en fait de définir chacun des tokens qui défini le langage comme case class. <br />
On verra un peu plus loin pour l'implémentation de cette fonction tokenize
  </aside>
</section>

<section data-speaker="marc">
  <h4>L'analyse syntaxique</h4>
  <img src="./images/parsing_2.png" />
  <aside class="notes">
La ou le lexing va juste vérifier que tous les mots font bien parti du langage, le parsing va par contre verifier que les mots sont dans le bon sens <br />
Ces mots sont hierarchiés par le parseur pour former un AST <br />
L'AST est donc une structure en arbre qui va être exploitable par le code de manière plus simple qu'avec une String ou un Seq[Token] <br />
N'importe quel language utilise ça (y a un AST pour Java, pour Scala, etc.) y a rien de révolutionnaire et ça n'est pas du tout propre à Spark
  </aside>
</section>

<section data-speaker="marc">
  <pre class="bigger-code scala"><code data-trim data-noescape>
trait AST

case class Table(name: String) extends AST

case class Select(
  projections: Seq[Expression],
  filter: Option[Expression],
  relations: Seq[Table]
) extends AST
  </code></pre>
  <pre class="bigger-code scala fragment"><code data-trim data-noescape>
trait Expression extends AST {

  def evaluate(row: Row): AnyRef

  def needColumns: Seq[ColumnName]

}

// ...
  </code></pre>

  <pre class="bigger-code scala fragment"><code data-trim data-noescape>
def parse: Seq[Token] => AST
  </code></pre>
  <aside class="notes">
Au niveau du code, idem, c'est pas très compliqué <br />
On va avoir un trait par noeud de l'AST, et on voit que le SELECT est une phrase de notre langage, et c'est lui qui nous sert de racine <br />
On distingue aussi une sous catégorie qui sont les expressions (en gros, ce que l'on va retrouver dans les filtres, etc.)
  </aside>
</section>

<section data-speaker="marc">
  <h3>Implémentations&nbsp;:</h3>
  <div>
    <div class="left-column">
      <img src="./images/scala-logo.png" />
      <h4 style="text-align: center;">Parser Combinators</h4>
      <pre class="bigger-code scala"><code data-trim data-noescape>
object Parser extends Parsers {

  override type Elem = l.Token

  def literal: Parser[e.Literal] =
    { number | text }

  def from = l.From()
               ~> tables

// ...
      </code></pre>
    </div>
    <div class="right-column fragment">
      <img style="width: 50%; height: 50%;" src="./images/antlr-logo.png" />
      <pre class="bigger-code"><code data-trim data-noescape>
statement
  : query
  | USE db=identifier
  | CREATE DATABASE
    (IF NOT EXISTS)? identifier
    (COMMENT comment=STRING)?
      locationSpec?
    (WITH DBPROPERTIES
      tablePropertyList)
      </code></pre>
    </div>
  </div>
  <aside class="notes">
Pour nous, les Parser Combinator s'était suffisant <br />
Mais ANTLR c'est plus sérieux pour des langages beaucoup plus complexes (type SQL ANSI, etc.)
  </aside>
</section>

<section data-speaker="marc">
  <h3>Ce qu'il faut retenir&nbsp;:</h3>
  <ul>
    <li>Une étape d'analyse lexicale : <code>String => Seq[Token]</code></li>
    <li>Une étape d'analyse syntaxique : <code>Seq[Token] => AST</code></li>
    <li>L'<em>AST</em> est une structure qui a du sens en termes de types : exploitable par le code</li>
    <li><strong>ANTLR est un standard</strong> pour des langages complexes, et c'est ce qu'utilise <em>Apache Spark</em>. </li>
  </ul>
  <p><span class="fragment">Notre requête SQL étant parsée&hellip; </span><span class="fragment">Il n'y plus qu'à l'executer&nbsp;!</span></p>
</section>
